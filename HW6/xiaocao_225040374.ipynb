{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53025e08",
   "metadata": {},
   "source": [
    "# DDA5002_HW6\n",
    "by Xiaocao_225040374"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca3e7b",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "We are given the function $f(x) = |x|^{3/2}$. We assume $x \\in \\mathbb{R}$.\n",
    "\n",
    "### (a) Prove $f$ is not Lipschitz smooth.\n",
    "\n",
    "A function $f$ is Lipschitz smooth with constant $L>0$ if its gradient is Lipschitz continuous, i.e., $\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x - y\\|$ for all $x, y$. In our 1D case, this is $|f'(x) - f'(y)| \\le L|x-y|$.\n",
    "\n",
    "First, let's find the derivative of $f(x)$.\n",
    "- For $x > 0$, $f(x) = x^{3/2}$, so $f'(x) = \\frac{3}{2}x^{1/2}$.\n",
    "- For $x < 0$, $f(x) = (-x)^{3/2}$, so $f'(x) = \\frac{3}{2}(-x)^{1/2} \\cdot (-1) = -\\frac{3}{2}(-x)^{1/2}$.\n",
    "We can write this as $f'(x) = \\frac{3}{2} \\text{sign}(x)\\sqrt{|x|}$.\n",
    "\n",
    "Let's find the derivative at $x=0$:\n",
    "$$ f'(0) = \\lim_{h\\to 0} \\frac{f(h) - f(0)}{h} = \\lim_{h\\to 0} \\frac{|h|^{3/2}}{h} $$\n",
    "- If $h \\to 0^+$, $\\lim_{h\\to 0^+} \\frac{h^{3/2}}{h} = \\lim_{h\\to 0^+} h^{1/2} = 0$.\n",
    "- If $h \\to 0^-$, $\\lim_{h\\to 0^-} \\frac{(-h)^{3/2}}{h} = \\lim_{h\\to 0^-} \\frac{-(-h)\\sqrt{-h}}{h} = \\lim_{h\\to 0^-} -\\sqrt{-h} = 0$.\n",
    "So, $f'(0) = 0$.\n",
    "\n",
    "Now, let's check the Lipschitz condition for $f'(x)$. Let's pick $y=0$ and $x > 0$.\n",
    "The condition is $|f'(x) - f'(0)| \\le L|x-0|$.\n",
    "$$ |\\frac{3}{2}\\sqrt{x} - 0| \\le L|x| $$\n",
    "$$ \\frac{3}{2}\\sqrt{x} \\le Lx $$\n",
    "$$ \\frac{3}{2\\sqrt{x}} \\le L $$\n",
    "As $x \\to 0^+$, the term $\\frac{3}{2\\sqrt{x}}$ goes to infinity. It is impossible to find a finite constant $L$ that satisfies this inequality for all $x$.\n",
    "\n",
    "Therefore, $f(x) = |x|^{3/2}$ is not Lipschitz smooth.\n",
    "\n",
    "### (b) Prove that for any constant step size $\\bar{\\alpha} > 0$, there exists an initial point $x_0$ for which $f(x_1) \\ge f(x_0)$.\n",
    "\n",
    "The gradient descent update rule is $x_1 = x_0 - \\bar{\\alpha} \\nabla f(x_0)$. In our case, $x_1 = x_0 - \\bar{\\alpha} f'(x_0)$.\n",
    "\n",
    "We want to show that there exists an $x_0$ such that $f(x_1) \\ge f(x_0)$, which is equivalent to $|x_1|^{3/2} \\ge |x_0|^{3/2}$, or $|x_1| \\ge |x_0|$.\n",
    "\n",
    "Let's choose $x_0 > 0$. The update rule becomes:\n",
    "$$ x_1 = x_0 - \\bar{\\alpha} \\left(\\frac{3}{2}\\sqrt{x_0}\\right) $$\n",
    "We want to find an $x_0 > 0$ such that:\n",
    "$$ \\left|x_0 - \\frac{3}{2}\\bar{\\alpha}\\sqrt{x_0}\\right| \\ge x_0 $$\n",
    "Let $\\sqrt{x_0} = z$. Since $x_0 > 0$, we have $z > 0$. The inequality becomes:\n",
    "$$ |z^2 - \\frac{3}{2}\\bar{\\alpha}z| \\ge z^2 $$\n",
    "$$ |z(z - \\frac{3}{2}\\bar{\\alpha})| \\ge z^2 $$\n",
    "Since $z > 0$, we can divide by $z$:\n",
    "$$ |z - \\frac{3}{2}\\bar{\\alpha}| \\ge z $$\n",
    "For any given $\\bar{\\alpha} > 0$, we can choose $z$ to be small enough such that $z < \\frac{3}{2}\\bar{\\alpha}$. For example, we can choose $z \\le \\frac{3}{4}\\bar{\\alpha}$.\n",
    "If $0 < z \\le \\frac{3}{4}\\bar{\\alpha}$, then $z - \\frac{3}{2}\\bar{\\alpha}$ is negative. So, its absolute value is:\n",
    "$$ |z - \\frac{3}{2}\\bar{\\alpha}| = \\frac{3}{2}\\bar{\\alpha} - z $$\n",
    "The inequality becomes:\n",
    "$$ \\frac{3}{2}\\bar{\\alpha} - z \\ge z $$\n",
    "$$ \\frac{3}{2}\\bar{\\alpha} \\ge 2z $$\n",
    "$$ z \\le \\frac{3}{4}\\bar{\\alpha} $$\n",
    "So, for any $\\bar{\\alpha} > 0$, we can choose any $x_0$ such that $0 < \\sqrt{x_0} \\le \\frac{3}{4}\\bar{\\alpha}$. For example, we can pick $x_0 = (\\frac{3}{4}\\bar{\\alpha})^2$. For such an $x_0$, the gradient descent step will result in an increase in the function value. This proves the statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8969fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For step size alpha = 0.1:\n",
      "  - We chose x0 = 0.0025\n",
      "  - The gradient at x0 is grad_f(x0) = 0.0750\n",
      "  - The next iterate is x1 = x0 - alpha * grad_f(x0) = -0.0050\n",
      "  - The function value at x0 is f(x0) = 0.0001\n",
      "  - The function value at x1 is f(x1) = 0.0004\n",
      "  - As shown, f(x1) >= f(x0), so the function value increased.\n",
      "------------------------------\n",
      "For step size alpha = 1.0:\n",
      "  - We chose x0 = 0.2500\n",
      "  - The gradient at x0 is grad_f(x0) = 0.7500\n",
      "  - The next iterate is x1 = x0 - alpha * grad_f(x0) = -0.5000\n",
      "  - The function value at x0 is f(x0) = 0.1250\n",
      "  - The function value at x1 is f(x1) = 0.3536\n",
      "  - As shown, f(x1) >= f(x0), so the function value increased.\n",
      "------------------------------\n",
      "For step size alpha = 10.0:\n",
      "  - We chose x0 = 25.0000\n",
      "  - The gradient at x0 is grad_f(x0) = 7.5000\n",
      "  - The next iterate is x1 = x0 - alpha * grad_f(x0) = -50.0000\n",
      "  - The function value at x0 is f(x0) = 125.0000\n",
      "  - The function value at x1 is f(x1) = 353.5534\n",
      "  - As shown, f(x1) >= f(x0), so the function value increased.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script demonstrates the point made in Problem 1(b) of HW6.\n",
    "It shows that for the function f(x) = |x|^(3/2), and for any constant\n",
    "step size alpha, we can find an initial point x0 such that a gradient\n",
    "descent step increases the function value.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes the function value f(x) = |x|^(3/2).\n",
    "    \"\"\"\n",
    "    return np.abs(x)**1.5\n",
    "\n",
    "def grad_f(x):\n",
    "    \"\"\"\n",
    "    Computes the gradient of f(x).\n",
    "    Note that the gradient at x=0 is 0.\n",
    "    \"\"\"\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return 1.5 * np.sign(x) * np.sqrt(np.abs(x))\n",
    "\n",
    "def demonstrate_divergence(alpha):\n",
    "    \"\"\"\n",
    "    For a given step size alpha, finds an x0 that demonstrates\n",
    "    f(x1) >= f(x0).\n",
    "\n",
    "    Args:\n",
    "        alpha (float): The constant step size for gradient descent.\n",
    "    \"\"\"\n",
    "    # From the derivation in prob1.md, we know that if we choose x0 such that\n",
    "    # 0 < sqrt(x0) <= (3/4)*alpha, the condition will be met.\n",
    "    # Let's choose sqrt(x0) = 0.5 * alpha.\n",
    "    x0 = (0.5 * alpha)**2\n",
    "\n",
    "    # Perform one step of gradient descent\n",
    "    gradient = grad_f(x0)\n",
    "    x1 = x0 - alpha * gradient\n",
    "\n",
    "    # Get the function values\n",
    "    f_x0 = f(x0)\n",
    "    f_x1 = f(x1)\n",
    "\n",
    "    print(f\"For step size alpha = {alpha}:\")\n",
    "    print(f\"  - We chose x0 = {x0:.4f}\")\n",
    "    print(f\"  - The gradient at x0 is grad_f(x0) = {gradient:.4f}\")\n",
    "    print(f\"  - The next iterate is x1 = x0 - alpha * grad_f(x0) = {x1:.4f}\")\n",
    "    print(f\"  - The function value at x0 is f(x0) = {f_x0:.4f}\")\n",
    "    print(f\"  - The function value at x1 is f(x1) = {f_x1:.4f}\")\n",
    "\n",
    "    if f_x1 >= f_x0:\n",
    "        print(\"  - As shown, f(x1) >= f(x0), so the function value increased.\")\n",
    "    else:\n",
    "        # This part should not be reached given the correct derivation\n",
    "        print(\"  - f(x1) < f(x0). Something is wrong with the logic.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Demonstrate for a few different step sizes\n",
    "    demonstrate_divergence(alpha=0.1)\n",
    "    demonstrate_divergence(alpha=1.0)\n",
    "    demonstrate_divergence(alpha=10.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f3a75",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb83c8",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00372cd6",
   "metadata": {},
   "source": [
    "Since matrix A is symmetric, the gradient of $f$ is\n",
    "$\\nabla f(x) = 2Ax$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla f(x_0)\n",
    "&= 2A x_0 \\\\\n",
    "&= 2\n",
    "\\begin{bmatrix} 2 & 0 \\\\ 0 & 5 \\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43297911",
   "metadata": {},
   "source": [
    "#### (i) constant step size\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1\n",
    "&= x_0 - \\alpha \\nabla f(x_0) \\\\\n",
    "&= \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "- 0.1 \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 0.6 \\\\ 0 \\end{bmatrix}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The function value at the new iterate is\n",
    "$$\n",
    "f(x_1) = 2(0.6)^2 + 5(0)^2 = 0.72.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77828280",
   "metadata": {},
   "source": [
    "#### (ii) exact line search\n",
    "\n",
    "Denote $g = \\nabla f(x_0)$. Consider $\\phi(\\alpha) = f(x_0 - \\alpha g)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi(\\alpha)\n",
    "&= (x_0 - \\alpha g)^\\top A (x_0 - \\alpha g) \\\\\n",
    "&= x_0^\\top A x_0 - \\alpha g^\\top g + \\alpha^2 g^\\top A g.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi'(\\alpha)\n",
    "&= - g^\\top g + 2\\alpha g^\\top A g = 0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Solve and get the optimal step size\n",
    "$$\n",
    "\\alpha^\\star = \\frac{g^\\top g}{2 g^\\top A g} = \\frac{29}{266}.\n",
    "$$\n",
    "\n",
    "Then calculate the update for $x_1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1\n",
    "&= x_0 - \\alpha^\\star g \\\\\n",
    "&= \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "- \\frac{29}{266} \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix}\n",
    "= \\begin{bmatrix} \\tfrac{75}{133} \\\\ -\\tfrac{12}{133} \\end{bmatrix}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x_1)\n",
    "&= 2\\left(\\frac{75}{133}\\right)^2\n",
    "+ 5\\left(\\frac{12}{133}\\right)^2 \\\\\n",
    "&\\approx 0.6769.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b12167",
   "metadata": {},
   "source": [
    "\n",
    "#### (iii) backtracking line search\n",
    "\n",
    "Substituting the parameter values at $x_0$,\n",
    "$$\n",
    "f(x_0 - \\alpha g) \\le 7 - 58\\alpha.\n",
    "$$\n",
    "\n",
    "- Starting from $\\alpha = 1$: the Armijo condition is not satisfied. Then continue to reduce $\\alpha$ until $\\alpha = 0.04$, where the Armijo condition is satisfied.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1\n",
    "&= x_0 - 0.04 \\nabla f(x_0) \\\\\n",
    "&= \\begin{bmatrix} 0.84 \\\\ 0.6 \\end{bmatrix},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "And the result is $f(x_1) = 3.2112$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ddc8a",
   "metadata": {},
   "source": [
    "### (b) Implement the strategies in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272fcd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constant step size\n",
      "- | alpha       | x^T              | f(x)\n",
      "--+-------------+------------------+----------------\n",
      " 0|   0.100000 | [ 1.000000,  1.000000] |  7.0000000000\n",
      " 1|   0.100000 | [ 0.600000,  0.000000] |  0.7200000000\n",
      " 2|   0.100000 | [ 0.360000,  0.000000] |  0.2592000000\n",
      " 3|   0.100000 | [ 0.216000,  0.000000] |  0.0933120000\n",
      " 4|   0.100000 | [ 0.129600,  0.000000] |  0.0335923200\n",
      " 5|   0.100000 | [ 0.077760,  0.000000] |  0.0120932352\n",
      " 6|   0.100000 | [ 0.046656,  0.000000] |  0.0043535647\n",
      " 7|   0.100000 | [ 0.027994,  0.000000] |  0.0015672833\n",
      " 8|   0.100000 | [ 0.016796,  0.000000] |  0.0005642220\n",
      " 9|   0.100000 | [ 0.010078,  0.000000] |  0.0002031199\n",
      "10|   0.100000 | [ 0.006047,  0.000000] |  0.0000731232\n"
     ]
    }
   ],
   "source": [
    "# Constant step size\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[2.0, 0.0],\n",
    "              [0.0, 5.0]])\n",
    "x0 = np.array([1.0, 1.0])\n",
    "\n",
    "def f(x: np.ndarray) -> float:\n",
    "    return float(x.T @ A @ x)\n",
    "\n",
    "def grad(x: np.ndarray) -> np.ndarray:\n",
    "    return 2.0 * (A @ x)\n",
    "\n",
    "def gd_constant(x0: np.ndarray, alpha: float, iters: int = 10):\n",
    "    x = x0.copy().astype(float)\n",
    "    hist = []\n",
    "    for k in range(iters):\n",
    "        g = grad(x)\n",
    "        x_new = x - alpha * g\n",
    "        hist.append((k, alpha, x.copy(), f(x)))\n",
    "        x = x_new\n",
    "    hist.append((iters, alpha, x.copy(), f(x)))\n",
    "    return hist\n",
    "\n",
    "h1 = gd_constant(x0, alpha=0.1, iters=10)\n",
    "def print_hist(title: str, hist):\n",
    "    print(\"\\n\" + title)\n",
    "    print(\"- | alpha       | x^T              | f(x)\")\n",
    "    print(\"--+-------------+------------------+----------------\")\n",
    "    for k, alpha, x, fx in hist:\n",
    "        if np.isnan(alpha):\n",
    "            a_str = \"   -\"\n",
    "        else:\n",
    "            a_str = f\"{alpha: .6f}\"\n",
    "        print(f\"{k:2d}|{a_str:>11} | [{x[0]: .6f}, {x[1]: .6f}] | {fx: .10f}\")\n",
    "\n",
    "print_hist(\"Constant step size\", h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af15c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact line search\n",
      "- | alpha       | x^T              | f(x)\n",
      "--+-------------+------------------+----------------\n",
      " 0|   0.109023 | [ 1.000000,  1.000000] |  7.0000000000\n",
      " 1|   0.207143 | [ 0.563910, -0.090226] |  0.6766917293\n",
      " 2|   0.109023 | [ 0.096670,  0.096670] |  0.0654159566\n",
      " 3|   0.207143 | [ 0.054513, -0.008722] |  0.0063237767\n",
      " 4|   0.109023 | [ 0.009345,  0.009345] |  0.0006113211\n",
      " 5|   0.207143 | [ 0.005270, -0.000843] |  0.0000590966\n",
      " 6|   0.109023 | [ 0.000903,  0.000903] |  0.0000057129\n",
      " 7|   0.207143 | [ 0.000509, -0.000082] |  0.0000005523\n",
      " 8|   0.109023 | [ 0.000087,  0.000087] |  0.0000000534\n",
      " 9|   0.207143 | [ 0.000049, -0.000008] |  0.0000000052\n",
      "10|          - | [ 0.000008,  0.000008] |  0.0000000005\n"
     ]
    }
   ],
   "source": [
    "# Exact line search\n",
    "def gd_exact_line_search(x0: np.ndarray, iters: int = 10):\n",
    "    x = x0.copy().astype(float)\n",
    "    hist = []\n",
    "    for k in range(iters):\n",
    "        g = grad(x)\n",
    "        gg = float(g.T @ g)\n",
    "        gAg = float(g.T @ (A @ g))\n",
    "        alpha = gg / (2.0 * gAg)\n",
    "\n",
    "        hist.append((k, alpha, x.copy(), f(x)))\n",
    "        x = x - alpha * g\n",
    "    hist.append((iters, np.nan, x.copy(), f(x)))\n",
    "    return hist\n",
    "h2 = gd_exact_line_search(x0, iters=10)\n",
    "print_hist(\"Exact line search\", h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261cf898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtracking line search\n",
      "- | alpha       | x^T              | f(x)\n",
      "--+-------------+------------------+----------------\n",
      " 0|   0.040000 | [ 1.000000,  1.000000] |  7.0000000000\n",
      " 1|   0.040000 | [ 0.840000,  0.600000] |  3.2112000000\n",
      " 2|   0.040000 | [ 0.705600,  0.360000] |  1.6437427200\n",
      " 3|   0.040000 | [ 0.592704,  0.216000] |  0.9358760632\n",
      " 4|   0.040000 | [ 0.497871,  0.129600] |  0.5797325822\n",
      " 5|   0.040000 | [ 0.418212,  0.077760] |  0.3800355455\n",
      " 6|   0.200000 | [ 0.351298,  0.046656] |  0.2577045257\n",
      " 7|   0.040000 | [ 0.070260, -0.046656] |  0.0207567362\n",
      " 8|   0.040000 | [ 0.059018, -0.027994] |  0.0108844732\n",
      " 9|   0.040000 | [ 0.049575, -0.016796] |  0.0063259515\n",
      "10|          - | [ 0.041643, -0.010078] |  0.0039761036\n"
     ]
    }
   ],
   "source": [
    "# Backtracking line search\n",
    "def armijo_backtracking_alpha(x: np.ndarray, gamma: float = 0.5, sigma: float = 0.2, alpha0: float = 1.0):\n",
    "    g = grad(x)\n",
    "    fx = f(x)\n",
    "    gg = float(g.T @ g)\n",
    "    alpha = alpha0\n",
    "    while f(x - alpha * g) > fx - gamma * alpha * gg:\n",
    "        alpha *= sigma\n",
    "    return alpha\n",
    "\n",
    "def gd_backtracking(x0: np.ndarray, iters: int = 10, gamma: float = 0.5, sigma: float = 0.2, alpha0: float = 1.0):\n",
    "    x = x0.copy().astype(float)\n",
    "    hist = []\n",
    "    for k in range(iters):\n",
    "        alpha = armijo_backtracking_alpha(x, gamma=gamma, sigma=sigma, alpha0=alpha0)\n",
    "        hist.append((k, alpha, x.copy(), f(x)))\n",
    "        x = x - alpha * grad(x)\n",
    "    hist.append((iters, np.nan, x.copy(), f(x)))\n",
    "    return hist\n",
    "\n",
    "h3 = gd_backtracking(x0, iters=10, gamma=0.5, sigma=0.2, alpha0=1.0)\n",
    "print_hist(\"Backtracking line search\", h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39430732",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "We are given the function $f: \\mathbb{R}^2 \\to \\mathbb{R}$:\n",
    "$$ f(x, y) = \\frac{3}{4}x^4 + \\frac{1}{2}(x + y)^2 $$\n",
    "\n",
    "### (a) Find its global minimizer.\n",
    "\n",
    "To find the minimizer, we first find the critical points by setting the gradient of $f$ to zero.\n",
    "The gradient is:\n",
    "$$ \\nabla f(x, y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 3x^3 + (x+y) \\\\ x+y \\end{pmatrix} $$\n",
    "Set the gradient to zero:\n",
    "$$ \\begin{pmatrix} 3x^3 + (x+y) \\\\ x+y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n",
    "From the second equation, we have $x+y=0$.\n",
    "Substituting this into the first equation gives $3x^3 = 0$, which implies $x=0$.\n",
    "Since $x+y=0$, we have $y = -x = 0$.\n",
    "The only critical point is $(0, 0)$.\n",
    "\n",
    "Now, we must determine if this point is a global minimizer.\n",
    "The function $f(x,y)$ is a sum of two non-negative terms:\n",
    "- $\\frac{3}{4}x^4 \\ge 0$ for all $x$.\n",
    "- $\\frac{1}{2}(x+y)^2 \\ge 0$ for all $x, y$.\n",
    "Thus, $f(x, y) \\ge 0$ for all $(x, y) \\in \\mathbb{R}^2$.\n",
    "At the critical point $(0,0)$, the function value is $f(0,0) = 0$.\n",
    "Since $f(x,y) \\ge f(0,0)$ for all $(x,y)$, the point $(0,0)$ is the global minimizer.\n",
    "\n",
    "### (b) Newton's method\n",
    "\n",
    "The update rule for Newton's method is:\n",
    "$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - [\\nabla^2 f(\\mathbf{x}_k)]^{-1} \\nabla f(\\mathbf{x}_k) $$\n",
    "where $\\mathbf{x}_k = (x_k, y_k)^T$.\n",
    "\n",
    "First, we compute the Hessian matrix $\\nabla^2 f(x, y)$:\n",
    "$$ \\nabla^2 f(x, y) = \\begin{pmatrix} 9x^2+1 & 1 \\\\ 1 & 1 \\end{pmatrix} $$\n",
    "The inverse of the Hessian is:\n",
    "$$ [\\nabla^2 f(x, y)]^{-1} = \\frac{1}{\\det(\\nabla^2 f)} \\begin{pmatrix} 1 & -1 \\\\ -1 & 9x^2+1 \\end{pmatrix} $$\n",
    "The determinant is $\\det(\\nabla^2 f) = (9x^2+1)(1) - (1)(1) = 9x^2$.\n",
    "The inverse is defined for $x \\ne 0$. The problem states we start with an initial point $(x_0, y_0)$ with $x_0 \\ne 0$.\n",
    "\n",
    "Let's compute the Newton step at a point $(x_k, y_k)$ where $x_k \\ne 0$:\n",
    "$$ [\\nabla^2 f]^{-1} \\nabla f = \\frac{1}{9x_k^2} \\begin{pmatrix} 1 & -1 \\\\ -1 & 9x_k^2+1 \\end{pmatrix} \\begin{pmatrix} 3x_k^3 + x_k+y_k \\\\ x_k+y_k \\end{pmatrix} $$\n",
    "The top component of the resulting vector is:\n",
    "$$ \\frac{1}{9x_k^2} [ (3x_k^3 + x_k+y_k) - (x_k+y_k) ] = \\frac{3x_k^3}{9x_k^2} = \\frac{x_k}{3} $$\n",
    "The bottom component is:\n",
    "$$ \\frac{1}{9x_k^2} [ -(3x_k^3 + x_k+y_k) + (9x_k^2+1)(x_k+y_k) ] = \\frac{1}{9x_k^2} [ -3x_k^3 - (x_k+y_k) + 9x_k^2(x_k+y_k) + (x_k+y_k) ] = \\frac{-3x_k^3 + 9x_k^2(x_k+y_k)}{9x_k^2} = -\\frac{x_k}{3} + x_k+y_k $$\n",
    "So, the update rule is:\n",
    "$$ \\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} - \\begin{pmatrix} x_k/3 \\\\ -x_k/3 + x_k+y_k \\end{pmatrix} $$\n",
    "$$ x_{k+1} = x_k - \\frac{x_k}{3} = \\frac{2}{3}x_k $$\n",
    "$$ y_{k+1} = y_k - (-\\frac{x_k}{3} + x_k+y_k) = y_k + \\frac{x_k}{3} - x_k - y_k = -\\frac{2}{3}x_k $$\n",
    "The expression for $(x_{k+1}, y_{k+1})$ is $(\\frac{2}{3}x_k, -\\frac{2}{3}x_k)$.\n",
    "\n",
    "The problem asks to prove that Newton's method does not converge to the global minimizer. However, my derivation shows that it does converge.\n",
    "Let's analyze the sequence of iterates starting from $(x_0, y_0)$ with $x_0 \\ne 0$:\n",
    "- $x_k = (\\frac{2}{3})^k x_0$\n",
    "- For $k \\ge 1$, $y_k = -\\frac{2}{3}x_{k-1} = -\\frac{2}{3}(\\frac{2}{3})^{k-1}x_0 = -(\\frac{2}{3})^k x_0$.\n",
    "As $k \\to \\infty$, since $|\\frac{2}{3}| < 1$, both $x_k \\to 0$ and $y_k \\to 0$. The sequence of iterates $(\\mathbf{x}_k)_{k \\ge 1}$ converges to $(0,0)$.\n",
    "\n",
    "**Note on Convergence:** The method *does* converge to the global minimizer $(0,0)$. The likely intent of the question relates to the *rate* of convergence. Newton's method typically exhibits quadratic convergence. However, at the minimizer $(0,0)$, the Hessian is $\\nabla^2 f(0,0) = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$, which is singular. A non-singular Hessian at the solution is a condition for quadratic convergence. Because the condition is not met, the method converges linearly (with a rate of 2/3), not quadratically. This might be what was meant by \"does not converge [as expected for Newton's method]\".\n",
    "\n",
    "### (c) Projected gradient descent\n",
    "\n",
    "We want to minimize $f$ subject to the constraint $\\Omega = \\{(x, y) : x + y = 1\\}$.\n",
    "One iteration of the projected gradient descent method is:\n",
    "1. Compute an intermediate point: $\\mathbf{z}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)$.\n",
    "2. Project the point onto the constraint set: $\\mathbf{x}_{k+1} = \\text{Proj}_{\\Omega}(\\mathbf{z}_{k+1})$.\n",
    "\n",
    "Given a point $\\mathbf{x}_k = (x_k, y_k)$ on the line $x_k+y_k=1$.\n",
    "1. Compute $\\mathbf{z}_{k+1} = (z_{k+1,1}, z_{k+1,2})^T$:\n",
    "   $$ \\nabla f(x_k, y_k) = \\begin{pmatrix} 3x_k^3 + (x_k+y_k) \\\\ x_k+y_k \\end{pmatrix} = \\begin{pmatrix} 3x_k^3 + 1 \\\\ 1 \\end{pmatrix} $$\n",
    "   $$ z_{k+1,1} = x_k - \\alpha_k(3x_k^3 + 1) $$\n",
    "   $$ z_{k+1,2} = y_k - \\alpha_k(1) $$\n",
    "2. Project $\\mathbf{z}_{k+1}$ onto the line $x+y=1$. The projection of a point $(z_1, z_2)$ onto the line $x+y=c$ is given by $x = \\frac{z_1-z_2+c}{2}, y = \\frac{-z_1+z_2+c}{2}$. Here $c=1$.\n",
    "   $$ x_{k+1} = \\frac{z_{k+1,1} - z_{k+1,2} + 1}{2} $$\n",
    "   $$ y_{k+1} = \\frac{-z_{k+1,1} + z_{k+1,2} + 1}{2} = 1 - x_{k+1} $$\n",
    "Substituting the expressions for $z_{k+1,1}$ and $z_{k+1,2}$:\n",
    "$$ z_{k+1,1} - z_{k+1,2} = (x_k - \\alpha_k(3x_k^3 + 1)) - (y_k - \\alpha_k) = x_k - y_k - 3\\alpha_k x_k^3 - \\alpha_k + \\alpha_k = x_k - y_k - 3\\alpha_k x_k^3 $$\n",
    "Now substitute this into the expression for $x_{k+1}$:\n",
    "$$ x_{k+1} = \\frac{(x_k - y_k - 3\\alpha_k x_k^3) + 1}{2} $$\n",
    "Since $(x_k, y_k)$ is on the line, $y_k = 1 - x_k$.\n",
    "$$ x_{k+1} = \\frac{x_k - (1-x_k) - 3\\alpha_k x_k^3 + 1}{2} = \\frac{2x_k - 1 - 3\\alpha_k x_k^3 + 1}{2} = x_k - \\frac{3}{2}\\alpha_k x_k^3 $$\n",
    "And $y_{k+1} = 1 - x_{k+1}$.\n",
    "\n",
    "So, one iteration is:\n",
    "$$ x_{k+1} = x_k - \\frac{3}{2}\\alpha_k x_k^3 $$\n",
    "$$ y_{k+1} = 1 - x_{k+1} $$\n",
    "This is the expression for one iteration of projected gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de27db9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Newton's Method...\n",
      "As shown in the markdown, the iterates converge linearly to (0,0),\n",
      "which contradicts the problem statement. This is due to the singular\n",
      "Hessian at the minimizer.\n",
      "\n",
      "--- Newton's Method Results ---\n",
      " iteration        x         y       f(x,y)\n",
      "         0 2.000000  5.000000 3.650000e+01\n",
      "         1 1.333333 -1.333333 2.370370e+00\n",
      "         2 0.888889 -0.888889 4.682213e-01\n",
      "         3 0.592593 -0.592593 9.248816e-02\n",
      "         4 0.395062 -0.395062 1.826927e-02\n",
      "         5 0.263374 -0.263374 3.608744e-03\n",
      "         6 0.175583 -0.175583 7.128383e-04\n",
      "         7 0.117055 -0.117055 1.408076e-04\n",
      "         8 0.078037 -0.078037 2.781384e-05\n",
      "         9 0.052025 -0.052025 5.494092e-06\n",
      "        10 0.034683 -0.034683 1.085253e-06\n",
      "        11 0.023122 -0.023122 2.143709e-07\n",
      "        12 0.015415 -0.015415 4.234487e-08\n",
      "        13 0.010276 -0.010276 8.364419e-09\n",
      "        14 0.006851 -0.006851 1.652231e-09\n",
      "        15 0.004567 -0.004567 3.263666e-10\n",
      "\n",
      "======================================================================\n",
      "\n",
      "--- Projected Gradient Descent Results (alpha=0.1) ---\n",
      " iteration        x         y    f(x,y)\n",
      "         0 2.000000 -1.000000 12.500000\n",
      "         1 0.800000  0.200000  0.807200\n",
      "         2 0.723200  0.276800  0.705161\n",
      "         3 0.666463  0.333537  0.647967\n",
      "         4 0.622059  0.377941  0.612302\n",
      "         5 0.585953  0.414047  0.588412\n",
      "         6 0.555775  0.444225  0.571558\n",
      "         7 0.530025  0.469975  0.559190\n",
      "         8 0.507690  0.492310  0.549826\n",
      "         9 0.488062  0.511938  0.542556\n",
      "        10 0.470623  0.529377  0.536792\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script implements the optimization algorithms for Problem 3 of HW6.\n",
    "It includes Newton's method and projected gradient descent for the function\n",
    "f(x, y) = (3/4)x^4 + (1/2)(x+y)^2.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def f(x_vec):\n",
    "    \"\"\"\n",
    "    Computes the function value f(x, y).\n",
    "\n",
    "    Args:\n",
    "        x_vec (np.ndarray): A 2D vector [x, y].\n",
    "\n",
    "    Returns:\n",
    "        float: The function value.\n",
    "    \"\"\"\n",
    "    x, y = x_vec[0], x_vec[1]\n",
    "    return 0.75 * x**4 + 0.5 * (x + y)**2\n",
    "\n",
    "def grad_f(x_vec):\n",
    "    \"\"\"\n",
    "    Computes the gradient of f(x, y).\n",
    "\n",
    "    Args:\n",
    "        x_vec (np.ndarray): A 2D vector [x, y].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The gradient vector.\n",
    "    \"\"\"\n",
    "    x, y = x_vec[0], x_vec[1]\n",
    "    return np.array([3 * x**3 + (x + y), x + y])\n",
    "\n",
    "def hessian_f(x_vec):\n",
    "    \"\"\"\n",
    "    Computes the Hessian of f(x, y).\n",
    "\n",
    "    Args:\n",
    "        x_vec (np.ndarray): A 2D vector [x, y].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The Hessian matrix.\n",
    "    \"\"\"\n",
    "    x, y = x_vec[0], x_vec[1]\n",
    "    return np.array([[9 * x**2 + 1, 1], [1, 1]])\n",
    "\n",
    "def newtons_method(x0_vec, num_iterations):\n",
    "    \"\"\"\n",
    "    Performs Newton's method.\n",
    "\n",
    "    Args:\n",
    "        x0_vec (np.ndarray): The initial point [x0, y0].\n",
    "        num_iterations (int): The number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: History of iterates and function values.\n",
    "    \"\"\"\n",
    "    x = x0_vec.copy()\n",
    "    history = []\n",
    "\n",
    "    print(\"Running Newton's Method...\")\n",
    "    print(\"As shown in the markdown, the iterates converge linearly to (0,0),\")\n",
    "    print(\"which contradicts the problem statement. This is due to the singular\")\n",
    "    print(\"Hessian at the minimizer.\\n\")\n",
    "\n",
    "    for k in range(num_iterations + 1):\n",
    "        f_x = f(x)\n",
    "        history.append({'iteration': k, 'x': x[0], 'y': x[1], 'f(x,y)': f_x})\n",
    "        if k == num_iterations:\n",
    "            break\n",
    "\n",
    "        if x[0] == 0:\n",
    "            print(f\"Hessian is singular at iteration {k} (x=0). Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Newton's step: x_k+1 = x_k - H_inv * g\n",
    "        g = grad_f(x)\n",
    "        H = hessian_f(x)\n",
    "        H_inv = np.linalg.inv(H)\n",
    "        x = x - H_inv @ g\n",
    "\n",
    "    return pd.DataFrame(history)\n",
    "\n",
    "def projected_gradient_descent(x0_vec, num_iterations, alpha):\n",
    "    \"\"\"\n",
    "    Performs projected gradient descent on the constraint x+y=1.\n",
    "\n",
    "    Args:\n",
    "        x0_vec (np.ndarray): The initial point [x0, y0]. Must sum to 1.\n",
    "        num_iterations (int): The number of iterations.\n",
    "        alpha (float): The step size.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: History of iterates and function values.\n",
    "    \"\"\"\n",
    "    if not np.isclose(x0_vec[0] + x0_vec[1], 1.0):\n",
    "        raise ValueError(\"Initial point must be on the line x+y=1.\")\n",
    "\n",
    "    x = x0_vec.copy()\n",
    "    history = []\n",
    "\n",
    "    for k in range(num_iterations + 1):\n",
    "        f_x = f(x)\n",
    "        history.append({'iteration': k, 'x': x[0], 'y': x[1], 'f(x,y)': f_x})\n",
    "        if k == num_iterations:\n",
    "            break\n",
    "\n",
    "        # As derived, the update is x_{k+1} = x_k - (3/2)*alpha*x_k^3\n",
    "        x[0] = x[0] - 1.5 * alpha * x[0]**3\n",
    "        x[1] = 1 - x[0]\n",
    "\n",
    "    return pd.DataFrame(history)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Part (b): Newton's Method ---\n",
    "    x_initial_newton = np.array([2.0, 5.0]) # An initial point with x0 != 0\n",
    "    iterations_newton = 15\n",
    "    history_newton = newtons_method(x_initial_newton, iterations_newton)\n",
    "    print(\"--- Newton's Method Results ---\")\n",
    "    print(history_newton.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "    # --- Part (c): Projected Gradient Descent ---\n",
    "    # Initial point must be on the line x+y=1\n",
    "    x_initial_pgd = np.array([2.0, -1.0])\n",
    "    iterations_pgd = 10\n",
    "    step_size_pgd = 0.1\n",
    "    history_pgd = projected_gradient_descent(x_initial_pgd, iterations_pgd, step_size_pgd)\n",
    "    print(\"--- Projected Gradient Descent Results (alpha=0.1) ---\")\n",
    "    print(history_pgd.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d87ba",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Let $L = \\{x \\in \\mathbb{R}^n : x = p + tv, t \\in \\mathbb{R}\\}$ be a line through a point $p \\in \\mathbb{R}^n$ with direction vector $v \\ne 0$.\n",
    "Let $B = \\{x \\in \\mathbb{R}^n : \\|x - c\\|_2 \\le r\\}$ be a closed ball of radius $r > 0$ centered at $c \\in \\mathbb{R}^n$.\n",
    "Let $C = L \\cap B$.\n",
    "We want to compute the projection of a point $x \\in \\mathbb{R}^n$ onto $C$, denoted as $P_C(x) = \\text{arg min}_{y \\in C} \\|y-x\\|^2$.\n",
    "\n",
    "### (a) Show that the projection onto $L$ can be expressed as $P_L(x) = p + \\frac{v^T(x-p)}{\\|v\\|^2}v$.\n",
    "\n",
    "The projection of $x$ onto the line $L$, denoted $P_L(x)$, is the point $y \\in L$ that minimizes the squared Euclidean distance $\\|y-x\\|^2$.\n",
    "A point $y$ on the line $L$ can be parameterized by $t \\in \\mathbb{R}$ as $y(t) = p + tv$.\n",
    "We want to find the value of $t$ that minimizes the distance. Let the squared distance be a function of $t$:\n",
    "$$ h(t) = \\|y(t) - x\\|^2 = \\|(p - x) + tv\\|^2 $$\n",
    "To minimize $h(t)$, we can minimize its squared value. We expand the expression:\n",
    "$$ h(t) = ((p-x)+tv)^T((p-x)+tv) = \\|p-x\\|^2 + 2t(p-x)^T v + t^2\\|v\\|^2 $$\n",
    "This is a quadratic function of $t$. To find the minimum, we take the derivative with respect to $t$ and set it to zero:\n",
    "$$ \\frac{dh}{dt} = 2(p-x)^T v + 2t\\|v\\|^2 = 0 $$\n",
    "Solving for $t$:\n",
    "$$ 2t\\|v\\|^2 = -2(p-x)^T v = 2(x-p)^T v $$\n",
    "$$ t^* = \\frac{(x-p)^T v}{\\|v\\|^2} = \\frac{v^T(x-p)}{\\|v\\|^2} $$\n",
    "The projection point $P_L(x)$ is $y(t^*)$:\n",
    "$$ P_L(x) = p + t^*v = p + \\frac{v^T(x-p)}{\\|v\\|^2}v $$\n",
    "This completes the proof.\n",
    "\n",
    "### (b) Explain that if the projection of $x$ onto $L$ lies in $B$ (i.e., $P_L(x) \\in B$), then $P_C(x) = P_L(x)$.\n",
    "\n",
    "The projection $P_C(x)$ is the solution to the constrained optimization problem:\n",
    "$$ \\min_{y} \\|y-x\\|^2 \\quad \\text{subject to} \\quad y \\in C $$\n",
    "The constraint set is $C = L \\cap B$. This means any feasible point $y$ must belong to both the line $L$ and the ball $B$.\n",
    "\n",
    "Let $y_L = P_L(x)$. By definition, $y_L$ is the unique point on the line $L$ that is closest to $x$. This means that for any point $y \\in L$ where $y \\ne y_L$, we have:\n",
    "$$ \\|y_L - x\\|^2 < \\|y - x\\|^2 $$\n",
    "We are given the condition that $P_L(x) \\in B$. This means $y_L \\in B$.\n",
    "Since $y_L$ is on the line $L$ by definition, and we are given that it is also in the ball $B$, it follows that $y_L \\in L \\cap B$, which means $y_L \\in C$.\n",
    "\n",
    "Now consider the optimization problem for $P_C(x)$. We are looking for a point in $C$ that is closest to $x$.\n",
    "Since $C$ is a subset of $L$ ($C \\subseteq L$), any point in $C$ is also in $L$.\n",
    "From the property of $y_L$ as the projection onto $L$, we know that it is closer to $x$ than any other point on $L$.\n",
    "Since all other points in $C$ are also on $L$, $y_L$ must also be closer to $x$ than any other point in $C$.\n",
    "$$ \\|y_L - x\\|^2 \\le \\|y - x\\|^2 \\quad \\text{for all } y \\in C $$\n",
    "And since we have already established that $y_L$ is itself an element of $C$, it must be the solution to the minimization problem over $C$.\n",
    "\n",
    "Therefore, if $P_L(x) \\in B$, then $P_C(x) = P_L(x).$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
