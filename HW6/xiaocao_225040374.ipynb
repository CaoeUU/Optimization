{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53025e08",
   "metadata": {},
   "source": [
    "# DDA5002_HW5\n",
    "by Xiaocao_225040374"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83adefb6",
   "metadata": {},
   "source": [
    "# Problem 1 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a196ead",
   "metadata": {},
   "source": [
    "## (a) Proof of $f$ is not Lipschitz smooth.\n",
    "\n",
    "**Lipschitz smooth(1D case):**\n",
    "$$\\exists L > 0, s.t. \\|\\nabla f(x) - \\nabla f(y)\\| = \\|f'(x) - f'(y)\\| \\le L \\|x - y\\|$$\n",
    "\n",
    "**Derivative of $f$:**\n",
    "$$\n",
    "f'(x) = \n",
    "\\begin{cases}\n",
    "\\frac{3}{2} \\sqrt{x}, x > 0 \\\\\n",
    "-\\frac{3}{2} \\sqrt{-x}, x < 0 \\\\\n",
    "0, x = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Suppose that $f$ is Lipschitz smooth and pick $y = 0$ and $x > 0$, then:\n",
    "$$\n",
    "|f'(x) - f'(0)| \\le L|x-0|\n",
    "$$\n",
    "$$ |\\frac{3}{2}\\sqrt{x} - 0| \\le L|x| $$\n",
    "$$ \\frac{3}{2\\sqrt{x}} \\le L $$\n",
    "As $x \\to 0^+$, we have $L \\to +\\infty$, so $f$ is **not Lipschitz smooth**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103fa3d",
   "metadata": {},
   "source": [
    "## (b) Proof of $\\forall \\bar{\\alpha} > 0, \\exists x_0, \\text{s.t.} f(x_1) \\ge f(x_0)$\n",
    "\n",
    "**Update rule:**\n",
    "$$\n",
    "x_1 = x_0 - \\bar{\\alpha} \\nabla f(x_0) = x_0 - \\bar{\\alpha}f'(x_0)\n",
    "$$\n",
    "\n",
    "Then the condition becomes:\n",
    "$$ f(x_1) \\ge f(x_0) $$\n",
    "$$ |x_1|^{3/2} \\ge |x_0|^{3/2}$$\n",
    "$$|x_1| = |x_0 - \\bar{\\alpha}f'(x_0)| = |x_0 - \\frac{3}{2} \\bar{\\alpha} \\sqrt{|x_0|}| \\ge |x_0|$$\n",
    "\n",
    "Let $\\sqrt{|x_0|} = z$, then:\n",
    "$$ |z^2 - \\frac{3 \\bar{\\alpha}}{2} z| \\geq z^2 \\Rightarrow |z - \\frac{3 \\bar{\\alpha}}{2} | \\geq z$$\n",
    "\n",
    "Pick $z = \\frac{3 \\bar{\\alpha}}{4}$ and $x_0 = \\pm (\\frac{3}{4}\\bar{\\alpha})^2$ will suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f3a75",
   "metadata": {},
   "source": [
    "# Problem 2 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb83c8",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00372cd6",
   "metadata": {},
   "source": [
    "Since matrix A is symmetric, the gradient of $f$ is\n",
    "$\\nabla f(x) = 2Ax$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla f(x_0)\n",
    "&= 2A x_0 \\\\\n",
    "&= 2\n",
    "\\begin{bmatrix} 2 & 0 \\\\ 0 & 5 \\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43297911",
   "metadata": {},
   "source": [
    "### (i) constant step size\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1\n",
    "&= x_0 - \\alpha \\nabla f(x_0) \\\\\n",
    "&= \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "- 0.1 \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 0.6 \\\\ 0 \\end{bmatrix}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The function value at the new iterate is\n",
    "$$\n",
    "f(x_1) = 2(0.6)^2 + 5(0)^2 = 0.72.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77828280",
   "metadata": {},
   "source": [
    "### (ii) exact line search\n",
    "\n",
    "Denote $g = \\nabla f(x_0)$. Consider $\\phi(\\alpha) = f(x_0 - \\alpha g)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi(\\alpha)\n",
    "&= (x_0 - \\alpha g)^\\top A (x_0 - \\alpha g) \\\\\n",
    "&= x_0^\\top A x_0 - \\alpha g^\\top g + \\alpha^2 g^\\top A g.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi'(\\alpha)\n",
    "&= - g^\\top g + 2\\alpha g^\\top A g = 0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Solve and get the optimal step size\n",
    "$$\n",
    "\\alpha^\\star = \\frac{g^\\top g}{2 g^\\top A g} = \\frac{29}{266}.\n",
    "$$\n",
    "\n",
    "Then calculate the update for $x_1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1\n",
    "&= x_0 - \\alpha^\\star g \\\\\n",
    "&= \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "- \\frac{29}{266} \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix}\n",
    "= \\begin{bmatrix} \\tfrac{75}{133} \\\\ -\\tfrac{12}{133} \\end{bmatrix}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x_1)\n",
    "&= 2\\left(\\frac{75}{133}\\right)^2\n",
    "+ 5\\left(\\frac{12}{133}\\right)^2 \\\\\n",
    "&\\approx 0.6769.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b12167",
   "metadata": {},
   "source": [
    "### (iii) backtracking line search\n",
    "\n",
    "Substituting the parameter values at $x_0$,\n",
    "$$\n",
    "f(x_0 - \\alpha g) \\le 7 - 58\\alpha.\n",
    "$$\n",
    "\n",
    "- Starting from $\\alpha = 1$: the Armijo condition is not satisfied. Then continue to reduce $\\alpha$ until $\\alpha = 0.04$, where the Armijo condition is satisfied.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1\n",
    "&= x_0 - 0.04 \\nabla f(x_0) \\\\\n",
    "&= \\begin{bmatrix} 0.84 \\\\ 0.6 \\end{bmatrix},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "And the result is $f(x_1) = 3.2112$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ddc8a",
   "metadata": {},
   "source": [
    "## (b) Implement the strategies in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272fcd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constant step size\n",
      "- | alpha       | x^T              | f(x)\n",
      "--+-------------+------------------+----------------\n",
      " 0|   0.100000 | [ 1.000000,  1.000000] |  7.0000000000\n",
      " 1|   0.100000 | [ 0.600000,  0.000000] |  0.7200000000\n",
      " 2|   0.100000 | [ 0.360000,  0.000000] |  0.2592000000\n",
      " 3|   0.100000 | [ 0.216000,  0.000000] |  0.0933120000\n",
      " 4|   0.100000 | [ 0.129600,  0.000000] |  0.0335923200\n",
      " 5|   0.100000 | [ 0.077760,  0.000000] |  0.0120932352\n",
      " 6|   0.100000 | [ 0.046656,  0.000000] |  0.0043535647\n",
      " 7|   0.100000 | [ 0.027994,  0.000000] |  0.0015672833\n",
      " 8|   0.100000 | [ 0.016796,  0.000000] |  0.0005642220\n",
      " 9|   0.100000 | [ 0.010078,  0.000000] |  0.0002031199\n",
      "10|   0.100000 | [ 0.006047,  0.000000] |  0.0000731232\n"
     ]
    }
   ],
   "source": [
    "# Constant step size\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[2.0, 0.0],\n",
    "              [0.0, 5.0]])\n",
    "x0 = np.array([1.0, 1.0])\n",
    "\n",
    "def f(x: np.ndarray) -> float:\n",
    "    return float(x.T @ A @ x)\n",
    "\n",
    "def grad(x: np.ndarray) -> np.ndarray:\n",
    "    return 2.0 * (A @ x)\n",
    "\n",
    "def gd_constant(x0: np.ndarray, alpha: float, iters: int = 10):\n",
    "    x = x0.copy().astype(float)\n",
    "    hist = []\n",
    "    for k in range(iters):\n",
    "        g = grad(x)\n",
    "        x_new = x - alpha * g\n",
    "        hist.append((k, alpha, x.copy(), f(x)))\n",
    "        x = x_new\n",
    "    hist.append((iters, alpha, x.copy(), f(x)))\n",
    "    return hist\n",
    "\n",
    "h1 = gd_constant(x0, alpha=0.1, iters=10)\n",
    "def print_hist(title: str, hist):\n",
    "    print(\"\\n\" + title)\n",
    "    print(\"- | alpha       | x^T              | f(x)\")\n",
    "    print(\"--+-------------+------------------+----------------\")\n",
    "    for k, alpha, x, fx in hist:\n",
    "        if np.isnan(alpha):\n",
    "            a_str = \"   -\"\n",
    "        else:\n",
    "            a_str = f\"{alpha: .6f}\"\n",
    "        print(f\"{k:2d}|{a_str:>11} | [{x[0]: .6f}, {x[1]: .6f}] | {fx: .10f}\")\n",
    "\n",
    "print_hist(\"Constant step size\", h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af15c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact line search\n",
      "- | alpha       | x^T              | f(x)\n",
      "--+-------------+------------------+----------------\n",
      " 0|   0.109023 | [ 1.000000,  1.000000] |  7.0000000000\n",
      " 1|   0.207143 | [ 0.563910, -0.090226] |  0.6766917293\n",
      " 2|   0.109023 | [ 0.096670,  0.096670] |  0.0654159566\n",
      " 3|   0.207143 | [ 0.054513, -0.008722] |  0.0063237767\n",
      " 4|   0.109023 | [ 0.009345,  0.009345] |  0.0006113211\n",
      " 5|   0.207143 | [ 0.005270, -0.000843] |  0.0000590966\n",
      " 6|   0.109023 | [ 0.000903,  0.000903] |  0.0000057129\n",
      " 7|   0.207143 | [ 0.000509, -0.000082] |  0.0000005523\n",
      " 8|   0.109023 | [ 0.000087,  0.000087] |  0.0000000534\n",
      " 9|   0.207143 | [ 0.000049, -0.000008] |  0.0000000052\n",
      "10|          - | [ 0.000008,  0.000008] |  0.0000000005\n"
     ]
    }
   ],
   "source": [
    "# Exact line search\n",
    "def gd_exact_line_search(x0: np.ndarray, iters: int = 10):\n",
    "    x = x0.copy().astype(float)\n",
    "    hist = []\n",
    "    for k in range(iters):\n",
    "        g = grad(x)\n",
    "        gg = float(g.T @ g)\n",
    "        gAg = float(g.T @ (A @ g))\n",
    "        alpha = gg / (2.0 * gAg)\n",
    "\n",
    "        hist.append((k, alpha, x.copy(), f(x)))\n",
    "        x = x - alpha * g\n",
    "    hist.append((iters, np.nan, x.copy(), f(x)))\n",
    "    return hist\n",
    "h2 = gd_exact_line_search(x0, iters=10)\n",
    "print_hist(\"Exact line search\", h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261cf898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtracking line search\n",
      "- | alpha       | x^T              | f(x)\n",
      "--+-------------+------------------+----------------\n",
      " 0|   0.040000 | [ 1.000000,  1.000000] |  7.0000000000\n",
      " 1|   0.040000 | [ 0.840000,  0.600000] |  3.2112000000\n",
      " 2|   0.040000 | [ 0.705600,  0.360000] |  1.6437427200\n",
      " 3|   0.040000 | [ 0.592704,  0.216000] |  0.9358760632\n",
      " 4|   0.040000 | [ 0.497871,  0.129600] |  0.5797325822\n",
      " 5|   0.040000 | [ 0.418212,  0.077760] |  0.3800355455\n",
      " 6|   0.200000 | [ 0.351298,  0.046656] |  0.2577045257\n",
      " 7|   0.040000 | [ 0.070260, -0.046656] |  0.0207567362\n",
      " 8|   0.040000 | [ 0.059018, -0.027994] |  0.0108844732\n",
      " 9|   0.040000 | [ 0.049575, -0.016796] |  0.0063259515\n",
      "10|          - | [ 0.041643, -0.010078] |  0.0039761036\n"
     ]
    }
   ],
   "source": [
    "# Backtracking line search\n",
    "def armijo_backtracking_alpha(\n",
    "    x: np.ndarray, gamma: float = 0.5,\n",
    "    sigma: float = 0.2, alpha0: float = 1.0\n",
    "):\n",
    "    g = grad(x)\n",
    "    fx = f(x)\n",
    "    gg = float(g.T @ g)\n",
    "    alpha = alpha0\n",
    "    while f(x - alpha * g) > fx - gamma * alpha * gg:\n",
    "        alpha *= sigma\n",
    "    return alpha\n",
    "\n",
    "def gd_backtracking(\n",
    "    x0: np.ndarray, iters: int = 10,\n",
    "    gamma: float = 0.5, sigma: float = 0.2, alpha0: float = 1.0\n",
    "):\n",
    "    x = x0.copy().astype(float)\n",
    "    hist = []\n",
    "    for k in range(iters):\n",
    "        alpha = armijo_backtracking_alpha(x, gamma=gamma, sigma=sigma, alpha0=alpha0)\n",
    "        hist.append((k, alpha, x.copy(), f(x)))\n",
    "        x = x - alpha * grad(x)\n",
    "    hist.append((iters, np.nan, x.copy(), f(x)))\n",
    "    return hist\n",
    "\n",
    "h3 = gd_backtracking(x0, iters=10, gamma=0.5, sigma=0.2, alpha0=1.0)\n",
    "print_hist(\"Backtracking line search\", h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f84e5",
   "metadata": {},
   "source": [
    "# Problem 3 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23503254",
   "metadata": {},
   "source": [
    "## (a) The global minimizer is (0, 0), the derivations are as follows:\n",
    "\n",
    "The global minimizer must satisfy:\n",
    "$$ \\nabla f(x, y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} x^{\\frac{1}{3}} + (x+y) \\\\ x+y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, $$\n",
    "\n",
    "so:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    x^{\\frac{1}{3}} + (x + y) = 0 \\\\\n",
    "    x + y = 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "and the **unique** solution is $x = 0, y = 0$. Besides, $\\forall x > 0, y > 0, f(x, y) > f(0, 0) = 0$, which shows that $(0, 0)$ is **the unique global minimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b8f50",
   "metadata": {},
   "source": [
    "## (b) Newton's Method:\n",
    "\n",
    "### (i) Finding $\\mathbf{x}_{k+1}$\n",
    "\n",
    "The expression for $(x_{k+1}, y_{k+1})$ is:\n",
    "$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - [\\nabla^2 f(\\mathbf{x}_k)]^{-1} \\nabla f(\\mathbf{x}_k) $$\n",
    "where $\\mathbf{x}_k = (x_k, y_k)^T$.\n",
    "\n",
    "Now we will compute the components one by one:\n",
    "\n",
    "1. The inverse of Hessian matrix: \n",
    "$$[\\nabla^2 f(\\mathbf{x}_k)]^{-1} = \\begin{pmatrix} \\frac{1}{3}{x_k}^{-\\frac{2}{3}}+1 & 1 \\\\ 1 & 1 \\end{pmatrix}^{-1} = (3{x_k}^{\\frac{2}{3}}) \\begin{pmatrix} 1 & -1 \\\\ -1 & \\frac{1}{3}x_k^{-\\frac{2}{3}}+1 \\end{pmatrix}$$\n",
    "\n",
    "2. The first order derivative: \n",
    "$$\\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} {x_k}^\\frac{1}{3} + x_k + y_k \\\\ x_k + y_k \\\\ \\end{pmatrix}$$\n",
    "\n",
    "Put together, we have:\n",
    "$$\n",
    "[\\nabla^2 f]^{-1} \\nabla f = \\left(3x_k^{\\frac{2}{3}}\\right) \\begin{pmatrix} 1 & -1 \\\\ -1 & \\frac{1}{3}x_k^{-\\frac{2}{3}}+1 \\end{pmatrix} \\begin{pmatrix} x_k^{\\frac{1}{3}} + x_k+y_k \\\\ x_k+y_k \\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "    3 x_k \\\\\n",
    "    -2 x_k + y_k \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So finally we get the expression of $\\mathbf{x}_{k+1}$:\n",
    "$$ \\mathbf{x}_{k+1} = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} - \\begin{pmatrix}\n",
    "    3 x_k \\\\\n",
    "    -2 x_k + y_k \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    -2 x_k \\\\\n",
    "    2 x_k \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "### (ii) Proof of Non-Convergence\n",
    "\n",
    "Start from $(x_0, y_0)$ with $x_0 \\neq 0$, the sequence of iteration is:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    x_k = (-2)^{k} x_0 \\neq 0 \\\\\n",
    "    y_k = 2 x_{k-1} = 2(-2)^{k-1} x_0 \\neq 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For all $k$, we have $\\mathbf{x}_k \\neq \\mathbf{x}^* = \\mathbf{0}$, so Newton Iterations starting from a non-zero initial point never converges to the global minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39430732",
   "metadata": {},
   "source": [
    "## (c) Expression of projected gradient descent:\n",
    "One iteration of the projected gradient descent method:\n",
    "1. Compute an intermediate point: $\\mathbf{z}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)$.\n",
    "2. Project onto constraint: $\\mathbf{x}_{k+1} = \\text{Proj}_{\\Omega}(\\mathbf{z}_{k+1})$.\n",
    "\n",
    "For $\\mathbf{x}_k = (x_k, y_k)$ on $x_k+y_k=1$:\n",
    "$$ \\nabla f(x_k, y_k) = \\begin{pmatrix} x_k^{\\frac{1}{3}} + 1 \\\\ 1 \\end{pmatrix} $$\n",
    "$$ z_{k+1,1} = x_k - \\alpha_k(x_k^{\\frac{1}{3}} + 1) $$\n",
    "$$ z_{k+1,2} = y_k - \\alpha_k $$\n",
    "The projection of a point $(z_1, z_2)$ onto the line $x+y=c$ is $( (z_1-z_2+c)/2, (z_2-z_1+c)/2 )$. Here $c=1$.\n",
    "$$ x_{k+1} = \\frac{(x_k - \\alpha_k(x_k^{\\frac{1}{3}} + 1)) - (y_k - \\alpha_k) + 1}{2} $$\n",
    "Since $y_k = 1 - x_k$:\n",
    "$$ x_{k+1} = \\frac{x_k - (1-x_k) - \\alpha_k x_k^{\\frac{1}{3}} + 1}{2} = x_k - \\frac{\\alpha_k}{2} x_k^{\\frac{1}{3}} $$\n",
    "And $y_{k+1} = 1 - x_{k+1}$.\n",
    "The iteration is:\n",
    "$$ x_{k+1} = x_k - \\frac{\\alpha_k}{2} x_k^{\\frac{1}{3}} $$\n",
    "$$ y_{k+1} = 1 - x_{k+1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9d987",
   "metadata": {},
   "source": [
    "# Problem 4 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb67a05",
   "metadata": {},
   "source": [
    "## (a) Show that the projection onto $L$ can be expressed as $P_L(x) = p + \\frac{v^T(x-p)}{\\|v\\|^2}v$.\n",
    "\n",
    "The projection $P_L(x)$ minimizes $\\|y-x\\|^2$ for $y \\in L$.\n",
    "\n",
    "Let $y(t) = p + tv$. We minimize $h(t) = \\|(p - x) + tv\\|^2 = \\|p-x\\|^2 + 2t(p-x)^T v + t^2\\|v\\|^2$.\n",
    "\n",
    "Setting $\\frac{dh}{dt} = 2(p-x)^T v + 2t\\|v\\|^2 = 0$, we find $t^* = \\frac{(x-p)^T v}{\\|v\\|^2} = \\frac{v^T(x-p)}{\\|v\\|^2}$.\n",
    "\n",
    "Thus, $P_L(x) = p + t^*v = p + \\frac{v^T(x-p)}{\\|v\\|^2}v$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c1856",
   "metadata": {},
   "source": [
    "## (b) Explain that if the projection of $x$ onto $L$ lies in $B$ (i.e., $P_L(x) \\in B$), then $P_C(x) = P_L(x)$.\n",
    "\n",
    "Let $y_L = P_L(x)$. By definition, $y_L$ is the unique point on line $L$ closest to $x$.\n",
    "\n",
    "If $P_L(x) \\in B$, then $y_L \\in B$. Since $y_L \\in L$, it follows that $y_L \\in L \\cap B$, so $y_L \\in C$.\n",
    "\n",
    "For any $y \\in C$, we know $y \\in L$. By the definition of $P_L(x)$, $\\|y_L - x\\|^2 \\le \\|y - x\\|^2$ for all $y \\in L$.\n",
    "\n",
    "Since $C \\subseteq L$, this inequality also holds for all $y \\in C$.\n",
    "\n",
    "As $y_L \\in C$ and it is the closest point to $x$ among all points in $C$, $y_L$ must be $P_C(x)$.\n",
    "\n",
    "Thus, if $P_L(x) \\in B$, then $P_C(x) = P_L(x).$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
